# PR Benchmark Regression Detection
#
# This workflow runs Iai-Callgrind benchmarks on pull requests
# and compares results against the main branch baseline to detect
# performance regressions.
#
# Regression Threshold: 10% increase in CPU instructions
#
# The workflow:
#   - Checks out main branch first to establish baseline
#   - Runs benchmarks on main and saves baseline
#   - Checks out PR branch and runs benchmarks comparing to baseline
#   - Posts results as PR comment

name: Benchmark PR

on:
  pull_request:
    branches:
      - main
    paths:
      # Only run benchmarks when relevant code changes
      - 'src/**'
      - 'lambars-derive/**'
      - 'benches/**'
      - 'Cargo.toml'
      - 'Cargo.lock'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  # Use shared target directory for baseline comparison
  CARGO_TARGET_DIR: /tmp/benchmark-target

jobs:
  benchmark-pr:
    name: Benchmark Regression Check
    runs-on: ubuntu-22.04
    permissions:
      pull-requests: write
    steps:
      - name: Checkout PR branch first (to get Cargo.lock)
        uses: actions/checkout@v4

      - name: Get iai-callgrind version
        id: iai-version
        run: |
          IAI_VERSION=$(grep -A1 'name = "iai-callgrind"' Cargo.lock | grep version | head -1 | sed 's/.*"\(.*\)"/\1/')
          echo "version=$IAI_VERSION" >> $GITHUB_OUTPUT

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: nightly-2025-12-15

      - name: Install Valgrind
        run: |
          # Remove problematic Azure CLI repository that may return 403
          sudo rm -f /etc/apt/sources.list.d/azure-cli.list || true
          sudo apt-get update || true
          sudo apt-get install -y valgrind

      - name: Cache cargo bin
        uses: actions/cache@v4
        with:
          path: ~/.cargo/bin
          key: ${{ runner.os }}-cargo-bin-iai-callgrind-runner-${{ steps.iai-version.outputs.version }}

      - name: Install iai-callgrind-runner
        run: |
          if ! command -v iai-callgrind-runner &> /dev/null; then
            cargo install iai-callgrind-runner --version ${{ steps.iai-version.outputs.version }}
          fi

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-registry-

      - name: Checkout main branch for baseline
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Check if iai benchmarks exist in main
        id: check-benchmarks
        run: |
          if [ -d "benches/iai" ]; then
            echo "benchmarks_exist=true" >> $GITHUB_OUTPUT
            # Extract IAI benchmark names from Cargo.toml
            MAIN_BENCHMARKS=$(grep -B2 'path = "benches/iai/' Cargo.toml | grep 'name = ' | sed 's/.*"\(.*\)"/\1/' | tr '\n' ' ')
            echo "main_benchmarks=$MAIN_BENCHMARKS" >> $GITHUB_OUTPUT
            echo "Main branch IAI benchmarks: $MAIN_BENCHMARKS"
          else
            echo "benchmarks_exist=false" >> $GITHUB_OUTPUT
            echo "main_benchmarks=" >> $GITHUB_OUTPUT
            echo "::notice::Iai benchmarks not found in main branch. Skipping baseline comparison."
          fi

      - name: Run baseline benchmarks (main branch)
        if: steps.check-benchmarks.outputs.benchmarks_exist == 'true'
        run: |
          set -eo pipefail
          BENCHMARKS="${{ steps.check-benchmarks.outputs.main_benchmarks }}"
          echo "Running baseline benchmarks: $BENCHMARKS"
          FIRST=true
          for bench in $BENCHMARKS; do
            echo "Running $bench..."
            if [ "$FIRST" = true ]; then
              cargo bench --bench "$bench" -- --save-baseline=main 2>&1 | tee baseline_output.txt
              FIRST=false
            else
              cargo bench --bench "$bench" -- --save-baseline=main 2>&1 | tee -a baseline_output.txt
            fi
          done

      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          clean: false

      - name: Update iai-callgrind-runner for PR branch if needed
        run: |
          # Get iai-callgrind version from PR branch's Cargo.lock
          PR_IAI_VERSION=$(grep -A1 'name = "iai-callgrind"' Cargo.lock | grep version | head -1 | sed 's/.*"\(.*\)"/\1/')
          INSTALLED_VERSION=$(iai-callgrind-runner --version 2>/dev/null | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "none")

          echo "PR branch iai-callgrind version: $PR_IAI_VERSION"
          echo "Installed runner version: $INSTALLED_VERSION"

          if [ "$PR_IAI_VERSION" != "$INSTALLED_VERSION" ]; then
            echo "Version mismatch detected. Installing iai-callgrind-runner $PR_IAI_VERSION"
            cargo install iai-callgrind-runner --version "$PR_IAI_VERSION" --force
          else
            echo "Runner version matches, no update needed"
          fi

      - name: Get PR branch benchmark list
        id: pr-benchmarks
        run: |
          # Extract IAI benchmark names from PR branch's Cargo.toml
          PR_BENCHMARKS=$(grep -B2 'path = "benches/iai/' Cargo.toml | grep 'name = ' | sed 's/.*"\(.*\)"/\1/' | tr '\n' ' ')
          echo "pr_benchmarks=$PR_BENCHMARKS" >> $GITHUB_OUTPUT
          echo "PR branch IAI benchmarks: $PR_BENCHMARKS"

          # Find common benchmarks (exist in both main and PR)
          MAIN_BENCHMARKS="${{ steps.check-benchmarks.outputs.main_benchmarks }}"
          COMMON=""
          NEW_ONLY=""

          for bench in $PR_BENCHMARKS; do
            if echo "$MAIN_BENCHMARKS" | grep -qw "$bench"; then
              COMMON="$COMMON $bench"
            else
              NEW_ONLY="$NEW_ONLY $bench"
            fi
          done

          echo "common_benchmarks=$COMMON" >> $GITHUB_OUTPUT
          echo "new_benchmarks=$NEW_ONLY" >> $GITHUB_OUTPUT
          echo "Common benchmarks (with baseline): $COMMON"
          echo "New benchmarks (no baseline): $NEW_ONLY"

      - name: Run PR benchmarks with baseline comparison
        if: steps.check-benchmarks.outputs.benchmarks_exist == 'true'
        run: |
          set -eo pipefail
          COMMON_BENCHMARKS="${{ steps.pr-benchmarks.outputs.common_benchmarks }}"

          if [ -n "$COMMON_BENCHMARKS" ]; then
            echo "Running benchmarks with baseline comparison: $COMMON_BENCHMARKS"
            FIRST=true
            for bench in $COMMON_BENCHMARKS; do
              echo "Running $bench with baseline..."
              if [ "$FIRST" = true ]; then
                cargo bench --bench "$bench" -- --baseline=main 2>&1 | tee benchmark_output.txt
                FIRST=false
              else
                cargo bench --bench "$bench" -- --baseline=main 2>&1 | tee -a benchmark_output.txt
              fi
            done
          else
            echo "No common benchmarks found for baseline comparison"
            # Create empty file to avoid errors in later steps
            touch benchmark_output.txt
          fi

      - name: Run PR benchmarks without baseline (new benchmarks)
        run: |
          set -eo pipefail

          # If no baseline exists at all, run all PR benchmarks
          if [ "${{ steps.check-benchmarks.outputs.benchmarks_exist }}" == "false" ]; then
            PR_BENCHMARKS="${{ steps.pr-benchmarks.outputs.pr_benchmarks }}"
            echo "No baseline exists. Running all PR benchmarks: $PR_BENCHMARKS"
            FIRST=true
            for bench in $PR_BENCHMARKS; do
              echo "Running $bench (no baseline)..."
              if [ "$FIRST" = true ]; then
                cargo bench --bench "$bench" -- --save-baseline=pr 2>&1 | tee benchmark_output.txt
                FIRST=false
              else
                cargo bench --bench "$bench" -- --save-baseline=pr 2>&1 | tee -a benchmark_output.txt
              fi
            done
          else
            # Run only new benchmarks (not in main)
            NEW_BENCHMARKS="${{ steps.pr-benchmarks.outputs.new_benchmarks }}"
            if [ -n "$NEW_BENCHMARKS" ]; then
              echo "Running new benchmarks (no baseline): $NEW_BENCHMARKS"
              FIRST=true
              # Check if benchmark_output.txt exists from previous step
              if [ ! -f benchmark_output.txt ]; then
                FIRST=true
              else
                FIRST=false
              fi

              for bench in $NEW_BENCHMARKS; do
                echo "Running $bench (new benchmark)..."
                if [ "$FIRST" = true ]; then
                  cargo bench --bench "$bench" -- --save-baseline=pr 2>&1 | tee benchmark_output.txt
                  FIRST=false
                else
                  cargo bench --bench "$bench" -- --save-baseline=pr 2>&1 | tee -a benchmark_output.txt
                fi
              done
            else
              echo "No new benchmarks to run"
            fi
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-pr-results
          path: |
            baseline_output.txt
            benchmark_output.txt
          retention-days: 30

      - name: Check for regressions
        id: check-regression
        run: |
          # Skip regression check if no baseline exists
          if [ "${{ steps.check-benchmarks.outputs.benchmarks_exist }}" == "false" ]; then
            echo "regression_detected=false" >> $GITHUB_OUTPUT
            echo "no_baseline=true" >> $GITHUB_OUTPUT
            echo "No baseline to compare against (new benchmarks)"
            exit 0
          fi

          echo "no_baseline=false" >> $GITHUB_OUTPUT

          # Parse benchmark output and check for regressions > 10%
          # Iai-Callgrind outputs changes like "+12.34%" or "-5.67%"
          # Match any increase of 10% or more (with or without decimal)
          if grep -E '\+1[0-9](\.[0-9]+)?%|\+[2-9][0-9](\.[0-9]+)?%|\+[0-9]{3,}(\.[0-9]+)?%' benchmark_output.txt; then
            echo "regression_detected=true" >> $GITHUB_OUTPUT
            echo "::warning::Performance regression detected (>10% increase in instructions)"
          else
            echo "regression_detected=false" >> $GITHUB_OUTPUT
            echo "No significant performance regression detected"
          fi

      - name: Post benchmark results as PR comment
        if: github.event.pull_request.head.repo.full_name == github.repository
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const benchmarkOutput = fs.readFileSync('benchmark_output.txt', 'utf8');

            // Strip ANSI escape codes (handles both real escape sequences and ^[[ notation)
            const stripAnsi = (str) => str
              .replace(/\x1b\[[0-9;]*m/g, '')  // Real ANSI escape sequences
              .replace(/\^\[\[[0-9;]*m/g, '')  // ^[[ notation
              .replace(/\033\[[0-9;]*m/g, ''); // Octal notation

            // Extract relevant benchmark results (last 100 lines to avoid huge comments)
            const lines = benchmarkOutput.split('\n');
            const relevantLines = stripAnsi(lines.slice(-100).join('\n'));

            const regressionDetected = '${{ steps.check-regression.outputs.regression_detected }}' === 'true';
            const noBaseline = '${{ steps.check-regression.outputs.no_baseline }}' === 'true';

            let status;
            if (noBaseline) {
              status = ':information_source: New Benchmarks (no baseline to compare)';
            } else if (regressionDetected) {
              status = ':warning: Performance Regression Detected';
            } else {
              status = ':white_check_mark: No Performance Regression';
            }

            const prBenchmarks = '${{ steps.pr-benchmarks.outputs.pr_benchmarks }}'.trim();
            const commonBenchmarks = '${{ steps.pr-benchmarks.outputs.common_benchmarks }}'.trim();
            const newBenchmarks = '${{ steps.pr-benchmarks.outputs.new_benchmarks }}'.trim();

            let benchmarkInfo = '';
            if (noBaseline) {
              benchmarkInfo = `> Benchmarks (no baseline): ${prBenchmarks}`;
            } else {
              const parts = [];
              if (commonBenchmarks) {
                parts.push(`with baseline: ${commonBenchmarks}`);
              }
              if (newBenchmarks) {
                parts.push(`new: ${newBenchmarks}`);
              }
              benchmarkInfo = `> Benchmarks (${parts.join(', ')})`;
            }

            const body = `## Benchmark Results

            ${status}

            <details>
            <summary>Iai-Callgrind Benchmark Output</summary>

            \`\`\`
            ${relevantLines}
            \`\`\`

            </details>

            > Threshold: 10% increase in CPU instructions
            ${benchmarkInfo}
            `;

            // Find existing comment or create new one
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('## Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Fail on regression (optional - enable when ready)
        if: steps.check-regression.outputs.regression_detected == 'true'
        run: |
          echo "::error::Performance regression detected. Review the benchmark results above."
          # Uncomment the next line to fail the workflow on regression
          # exit 1
