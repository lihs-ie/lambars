name: Profiling Analysis

on:
  pull_request:
    paths:
      - 'src/**'
      - 'benches/**'
      - 'lambars-derive/**'
      - 'Cargo.toml'
      - 'Cargo.lock'
  workflow_dispatch:
    inputs:
      profile_type:
        description: 'Profile type to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - criterion
          - iai
          - api

concurrency:
  group: profiling-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # ==========================================================================
  # Criterion Benchmarks with Flamegraph
  # ==========================================================================
  criterion-profiling:
    name: Criterion + Flamegraph
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.profile_type == 'all' || github.event.inputs.profile_type == 'criterion' || github.event_name == 'pull_request' }}

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-profiling-${{ hashFiles('**/Cargo.lock') }}

      - name: Install perf and flamegraph tools
        run: |
          sudo apt-get update
          sudo apt-get install -y linux-tools-common linux-tools-generic linux-tools-$(uname -r) gawk || true
          # Allow perf for non-root users
          echo '1' | sudo tee /proc/sys/kernel/perf_event_paranoid
          echo '0' | sudo tee /proc/sys/kernel/kptr_restrict
          # Install inferno for flamegraph generation
          cargo install inferno

      - name: Run Criterion benchmarks with profiling
        run: |
          mkdir -p profiling-results/criterion

          # Build benchmarks in release mode
          cargo build --release --benches

          # Profile key benchmarks using perf
          for bench in effect_bench for_macro_bench control_bench; do
            echo "=== Profiling $bench ==="

            # Find the benchmark binary
            BENCH_BIN=$(find target/release/deps -name "${bench}-*" -type f -executable | head -1)
            if [ -n "$BENCH_BIN" ]; then
              # Run with perf to collect profile data
              sudo perf record -F 99 -g -o profiling-results/criterion/${bench}.perf.data \
                "$BENCH_BIN" --bench --profile-time 5 2>&1 | tee profiling-results/criterion/${bench}.log || true

              # Generate flamegraph and text analysis if perf data exists
              if [ -f "profiling-results/criterion/${bench}.perf.data" ]; then
                echo "=== Processing perf data for ${bench} ==="
                ls -la profiling-results/criterion/${bench}.perf.data

                # Generate folded stacks (intermediate format for analysis)
                sudo perf script -i profiling-results/criterion/${bench}.perf.data 2>&1 | \
                  inferno-collapse-perf > profiling-results/criterion/${bench}.folded 2>&1 || echo "collapse-perf returned non-zero for ${bench}"

                # Generate flamegraph from folded stacks (check file size with -s)
                if [ -s "profiling-results/criterion/${bench}.folded" ]; then
                  echo "=== Generating flamegraph for ${bench} ==="
                  echo "folded lines: $(wc -l < profiling-results/criterion/${bench}.folded)"

                  cat profiling-results/criterion/${bench}.folded | \
                    inferno-flamegraph > profiling-results/criterion/${bench}_flamegraph.svg 2>&1 || echo "flamegraph generation failed for ${bench}"

                  # Generate top functions summary (overhead by function)
                  echo "=== Generating top functions for ${bench} ==="
                  gawk -F';' '{
                    n=split($NF, parts, " ")
                    fname=parts[1]
                    for(i=2; i<n; i++) fname=fname" "parts[i]
                    samples=parts[n]
                    sum[fname]+=samples
                    total+=samples
                  } END {
                    for(f in sum) printf "%d %s\n", sum[f], f
                  }' profiling-results/criterion/${bench}.folded | \
                    sort -rn | head -50 > profiling-results/criterion/${bench}_top_functions.txt

                  echo "top_functions entries: $(wc -l < profiling-results/criterion/${bench}_top_functions.txt)"
                else
                  echo "WARNING: ${bench}.folded is empty or missing"
                fi

                # Cleanup large perf data
                rm -f profiling-results/criterion/${bench}.perf.data
              fi
            else
              echo "Benchmark binary not found for $bench"
            fi
          done

      - name: Generate Criterion summary
        run: |
          echo "# Criterion Profiling Results" > profiling-results/criterion/README.md
          echo "" >> profiling-results/criterion/README.md
          echo "## Flamegraphs" >> profiling-results/criterion/README.md
          echo "" >> profiling-results/criterion/README.md
          for svg in profiling-results/criterion/*.svg; do
            if [ -f "$svg" ]; then
              name=$(basename "$svg" .svg)
              echo "- [$name](./$name.svg)" >> profiling-results/criterion/README.md
            fi
          done

      - name: Upload Criterion profiling artifacts
        uses: actions/upload-artifact@v4
        with:
          name: criterion-profiling-${{ github.sha }}
          path: profiling-results/criterion/
          retention-days: 30

  # ==========================================================================
  # iai-callgrind with Detailed Analysis
  # ==========================================================================
  iai-profiling:
    name: iai-callgrind Analysis
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.profile_type == 'all' || github.event.inputs.profile_type == 'iai' || github.event_name == 'pull_request' }}

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Install Valgrind
        run: |
          sudo apt-get update
          sudo apt-get install -y valgrind

      - name: Install iai-callgrind-runner
        run: |
          # Get iai-callgrind version from Cargo.lock to ensure version match
          IAI_VERSION=$(grep -A1 'name = "iai-callgrind"' Cargo.lock | grep version | head -1 | sed 's/.*"\(.*\)"/\1/')
          echo "Installing iai-callgrind-runner version $IAI_VERSION"
          cargo install iai-callgrind-runner --version "$IAI_VERSION"

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-iai-${{ hashFiles('**/Cargo.lock') }}

      - name: Run iai-callgrind benchmarks
        run: |
          mkdir -p profiling-results/iai

          # Run iai benchmarks
          for bench in persistent_vector_iai effect_iai scenario_iai; do
            echo "=== Running $bench ==="
            cargo bench --bench $bench 2>&1 | tee profiling-results/iai/${bench}.log || true
          done

          # Copy callgrind output files
          find target -name "callgrind.out.*" -exec cp {} profiling-results/iai/ \; 2>/dev/null || true

      - name: Generate iai-callgrind annotations
        run: |
          cd profiling-results/iai
          for callgrind_file in callgrind.out.*; do
            if [ -f "$callgrind_file" ]; then
              echo "=== Annotating $callgrind_file ==="
              callgrind_annotate --auto=yes "$callgrind_file" > "${callgrind_file}.annotated.txt" 2>/dev/null || true
            fi
          done

      - name: Parse iai results for summary
        id: iai_summary
        run: |
          echo "# iai-callgrind Results" > profiling-results/iai/README.md
          echo "" >> profiling-results/iai/README.md

          # Extract key metrics from logs
          for log in profiling-results/iai/*.log; do
            if [ -f "$log" ]; then
              name=$(basename "$log" .log)
              echo "## $name" >> profiling-results/iai/README.md
              echo '```' >> profiling-results/iai/README.md
              # Extract instruction counts and cache metrics
              grep -E "(Instructions|L1|LL|RAM)" "$log" | head -50 >> profiling-results/iai/README.md || echo "No metrics found" >> profiling-results/iai/README.md
              echo '```' >> profiling-results/iai/README.md
              echo "" >> profiling-results/iai/README.md
            fi
          done

      - name: Upload iai-callgrind artifacts
        uses: actions/upload-artifact@v4
        with:
          name: iai-profiling-${{ github.sha }}
          path: profiling-results/iai/
          retention-days: 30

  # ==========================================================================
  # API Profiling Matrix
  # ==========================================================================
  api-profiling-matrix:
    name: API Profiling Matrix
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.profile_type == 'all' || github.event.inputs.profile_type == 'api' || github.event_name == 'pull_request' }}
    outputs:
      scenarios: ${{ steps.matrix.outputs.scenarios }}

    steps:
      - uses: actions/checkout@v4

      - name: Build scenario matrix
        id: matrix
        run: |
          python - <<'PY' >> "$GITHUB_OUTPUT"
          import glob
          import json
          import os

          scenarios = []
          for path in sorted(glob.glob("benches/api/benchmarks/scenarios/*.yaml")):
              if os.path.basename(path) == "matrix.yaml":
                  continue
              scenarios.append(os.path.basename(path))

          print("scenarios=" + json.dumps(scenarios))
          PY

  # ==========================================================================
  # API Profiling Build
  # ==========================================================================
  api-profiling-build:
    name: API Profiling Build
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.profile_type == 'all' || github.event.inputs.profile_type == 'api' || github.event_name == 'pull_request' }}

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Build API server and tools
        run: |
          set -euo pipefail
          cargo install inferno
          cd benches/api
          CARGO_PROFILE_RELEASE_DEBUG=true cargo build --release --bin task-management-benchmark-api
          cd ../..
          mkdir -p api-profiling-tools/bin
          cp benches/api/target/release/task-management-benchmark-api api-profiling-tools/bin/
          cp ~/.cargo/bin/inferno-collapse-perf ~/.cargo/bin/inferno-flamegraph api-profiling-tools/bin/

      - name: Upload profiling tools
        uses: actions/upload-artifact@v4
        with:
          name: api-profiling-tools-${{ github.sha }}
          path: api-profiling-tools/bin
          retention-days: 7
          if-no-files-found: warn

  # ==========================================================================
  # API Server Profiling during Benchmark
  # ==========================================================================
  api-profiling:
    name: API Server Profiling (${{ matrix.scenario }})
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.profile_type == 'all' || github.event.inputs.profile_type == 'api' || github.event_name == 'pull_request' }}
    needs: [api-profiling-matrix, api-profiling-build]
    strategy:
      fail-fast: false
      matrix:
        scenario: ${{ fromJSON(needs.api-profiling-matrix.outputs.scenarios) }}

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wrk yq linux-tools-common linux-tools-generic linux-tools-$(uname -r) gawk || true
          # Allow perf for non-root users
          echo '1' | sudo tee /proc/sys/kernel/perf_event_paranoid
          echo '0' | sudo tee /proc/sys/kernel/kptr_restrict

      - name: Download profiling tools
        uses: actions/download-artifact@v4
        with:
          name: api-profiling-tools-${{ github.sha }}
          path: api-profiling-tools/bin

      - name: Resolve scenario name
        id: scenario
        run: |
          scenario="${{ matrix.scenario }}"
          echo "name=${scenario%.yaml}" >> "$GITHUB_OUTPUT"

      - name: Start API server and profile
        working-directory: benches/api
        env:
          MIXED_SCRIPT: recursive
        run: |
          set -euo pipefail

          export PATH="${GITHUB_WORKSPACE}/api-profiling-tools/bin:${PATH}"
          API_BIN="${GITHUB_WORKSPACE}/api-profiling-tools/bin/task-management-benchmark-api"
          SCENARIO_FILE="benchmarks/scenarios/${{ matrix.scenario }}"
          SCENARIO_NAME="${{ steps.scenario.outputs.name }}"
          RESULTS_DIR="${GITHUB_WORKSPACE}/profiling-results/api/${SCENARIO_NAME}"

          PERF_PID=""
          SERVER_PID=""

          cleanup() {
            if [[ -n "${PERF_PID}" ]]; then
              sudo kill -INT "${PERF_PID}" 2>/dev/null || true
              sleep 2
            fi
            if [[ -n "${SERVER_PID}" ]]; then
              kill "${SERVER_PID}" 2>/dev/null || true
            fi
          }
          trap cleanup EXIT

          echo "=== Tool versions ==="
          perf --version || true
          wrk --version || true
          yq --version || true
          inferno-collapse-perf --version || true

          mkdir -p "${RESULTS_DIR}"

          # Allow perf
          echo '1' | sudo tee /proc/sys/kernel/perf_event_paranoid || true
          echo '0' | sudo tee /proc/sys/kernel/kptr_restrict || true

          if [[ ! -x "${API_BIN}" ]]; then
            echo "ERROR: API binary not found at ${API_BIN}"
            exit 1
          fi

          # Start server in background with in-memory mode (no external dependencies)
          STORAGE_MODE=in_memory CACHE_MODE=in_memory RUST_LOG=info "${API_BIN}" &
          SERVER_PID=$!

          # Wait for server to be ready (health check loop)
          echo "Waiting for API server to start..."
          for i in {1..30}; do
            if curl -sf http://localhost:3000/health > /dev/null 2>&1; then
              echo "API server is ready (attempt $i)"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "ERROR: API server failed to start within 30 seconds"
              kill $SERVER_PID 2>/dev/null || true
              exit 1
            fi
            echo "  Attempt $i/30: waiting..."
            sleep 1
          done

          # Setup test data
          cd benchmarks
          chmod +x setup_test_data.sh run_benchmark.sh
          API_URL=http://localhost:3000 ./setup_test_data.sh || true

          # Run perf record during benchmark
          # Use cpu-clock software event for VM compatibility and higher sampling rate
          echo "=== Starting perf profiling ==="
          sudo perf record -e cpu-clock -F 997 -p $SERVER_PID -g -o "${RESULTS_DIR}/perf.data" &
          PERF_PID=$!

          # Run benchmark for a single scenario (mixed runs a single script)
          API_URL=http://localhost:3000 ./run_benchmark.sh --scenario "${SCENARIO_FILE}"

          # Stop perf and show statistics
          sudo kill -INT $PERF_PID 2>/dev/null || true
          PERF_PID=""
          sleep 2

          # Show perf record stats
          echo "=== perf record statistics ==="
          ls -la "${RESULTS_DIR}/perf.data" 2>/dev/null || echo "perf.data not found"

          # Generate flamegraph
          echo "=== Running perf script ==="
          sudo perf script -i "${RESULTS_DIR}/perf.data" > "${RESULTS_DIR}/perf.script" 2>&1 || echo "perf script returned non-zero"

          if [ -s "${RESULTS_DIR}/perf.script" ]; then
            echo "=== Generating folded stacks ==="
            inferno-collapse-perf < "${RESULTS_DIR}/perf.script" > "${RESULTS_DIR}/stacks.folded" 2>&1 || echo "inferno-collapse-perf returned non-zero"

            if [ -s "${RESULTS_DIR}/stacks.folded" ]; then
              echo "=== Generating flamegraph ==="
              inferno-flamegraph < "${RESULTS_DIR}/stacks.folded" > "${RESULTS_DIR}/flamegraph.svg" 2>&1 || echo "inferno-flamegraph returned non-zero"
            else
              echo "WARNING: stacks.folded is empty"
            fi
          else
            echo "WARNING: perf.script is empty - perf may not have captured data"
          fi

      - name: Upload API profiling artifacts
        uses: actions/upload-artifact@v4
        with:
          name: api-profiling-${{ github.sha }}-${{ steps.scenario.outputs.name }}
          path: profiling-results/api/${{ steps.scenario.outputs.name }}/
          retention-days: 30
          if-no-files-found: warn

  # ==========================================================================
  # Aggregate Results and Post PR Comment
  # ==========================================================================
  summarize-profiling:
    name: Summarize Profiling Results
    runs-on: ubuntu-latest
    needs: [criterion-profiling, iai-profiling, api-profiling]
    if: always() && github.event_name == 'pull_request'
    permissions:
      pull-requests: write

    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: profiling-artifacts

      - name: Post profiling summary to PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            let body = '## :microscope: Profiling Analysis Results\n\n';
            body += `Commit: \`${context.sha.substring(0, 7)}\`\n\n`;

            // Check for Criterion results
            const criterionDir = 'profiling-artifacts/criterion-profiling-' + context.sha;
            if (fs.existsSync(criterionDir)) {
              body += '### :chart_with_upwards_trend: Criterion Flamegraphs\n\n';
              const svgFiles = fs.readdirSync(criterionDir).filter(f => f.endsWith('.svg'));
              if (svgFiles.length > 0) {
                body += `Found ${svgFiles.length} flamegraph(s). Download artifacts to view.\n\n`;
                body += '| Benchmark | Flamegraph |\n|-----------|------------|\n';
                for (const svg of svgFiles) {
                  const name = svg.replace('_flamegraph.svg', '');
                  body += `| ${name} | :white_check_mark: |\n`;
                }
                body += '\n';
              } else {
                body += 'No flamegraphs generated. Check workflow logs.\n\n';
              }
            }

            // Check for iai results
            const iaiDir = 'profiling-artifacts/iai-profiling-' + context.sha;
            if (fs.existsSync(iaiDir)) {
              body += '### :bar_chart: iai-callgrind Analysis\n\n';
              const logFiles = fs.readdirSync(iaiDir).filter(f => f.endsWith('.log'));

              if (logFiles.length > 0) {
                body += `Analyzed ${logFiles.length} benchmark(s). Download artifacts for detailed reports.\n\n`;
              } else {
                body += 'iai-callgrind analysis completed. Download artifacts for detailed reports.\n\n';
              }
            }

            // Check for API results (multiple artifacts)
            const artifactsRoot = 'profiling-artifacts';
            const apiPrefix = 'api-profiling-' + context.sha;
            const apiArtifacts = fs.existsSync(artifactsRoot)
              ? fs.readdirSync(artifactsRoot).filter(name => name.startsWith(apiPrefix))
              : [];

            function findFiles(dir, fileName, results = []) {
              if (!fs.existsSync(dir)) return results;
              const entries = fs.readdirSync(dir, { withFileTypes: true });
              for (const entry of entries) {
                const fullPath = path.join(dir, entry.name);
                if (entry.isDirectory()) {
                  findFiles(fullPath, fileName, results);
                } else if (entry.isFile() && entry.name === fileName) {
                  results.push(fullPath);
                }
              }
              return results;
            }

            if (apiArtifacts.length > 0) {
              let flamegraphs = 0;
              for (const dirName of apiArtifacts) {
                const dirPath = path.join(artifactsRoot, dirName);
                flamegraphs += findFiles(dirPath, 'flamegraph.svg').length;
              }

              body += '### :fire: API Server Flamegraphs\n\n';
              body += `Artifacts: ${apiArtifacts.length}, Flamegraphs: ${flamegraphs}\n\n`;
              if (flamegraphs > 0) {
                body += ':white_check_mark: Flamegraphs generated successfully. Download artifacts to view.\n\n';
              } else {
                body += ':warning: Flamegraph generation may have failed. Check workflow logs.\n\n';
              }
            }

            body += '<details>\n<summary>:package: Download Artifacts</summary>\n\n';
            body += `Download profiling artifacts from the [Actions run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}):\n`;
            body += '- `criterion-profiling-*` - Criterion benchmark flamegraphs\n';
            body += '- `iai-profiling-*` - iai-callgrind detailed analysis\n';
            body += '- `api-profiling-*` - API server flamegraphs\n\n';
            body += '</details>\n\n';

            body += `> :clock1: Profiling completed at: ${new Date().toISOString()}`;

            // Find or create comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('## :microscope: Profiling Analysis Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
